{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfa99527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.metrics import accuracy_score # Accuracy metrics \n",
    "import pickle \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import warnings\n",
    "import mediapipe as mp # Import mediapipe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Suppress specific sklearn warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17cff4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_detect_setup():\n",
    "\n",
    "    filename = '/Users/kailashambwani/Dropbox/PitchPerfect/body_models/face_model_gb.pkl'\n",
    "    with open(filename, 'rb') as f:\n",
    "        face_model = pickle.load(f)\n",
    "\n",
    "    filename = '/Users/kailashambwani/Dropbox/PitchPerfect/body_models/pose_model_rf.pkl'\n",
    "    with open(filename, 'rb') as f:\n",
    "        pose_model = pickle.load(f)\n",
    "        \n",
    "    return face_model, pose_model\n",
    "\n",
    "def feature_detect(frame, face_model, pose_model):\n",
    "    # Recolor Feed\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False        \n",
    "\n",
    "    # Make Detections\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "        results = holistic.process(image)\n",
    "\n",
    "    try:\n",
    "        # Extract Pose landmarks\n",
    "        pose = results.pose_landmarks.landmark\n",
    "        pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "\n",
    "        # Extract Face landmarks\n",
    "        face = results.face_landmarks.landmark\n",
    "        face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "\n",
    "#             # Concate rows\n",
    "#             row = pose_row+face_row\n",
    "\n",
    "        # Make face Detections\n",
    "        face_X = pd.DataFrame([face_row])\n",
    "        face_class = face_model.predict(face_X)[0]\n",
    "        face_prob = face_model.predict_proba(face_X)[0]\n",
    "#            print(face_class, face_prob)\n",
    "\n",
    "        # Make pose Detections\n",
    "        pose_X = pd.DataFrame([pose_row])\n",
    "        pose_class = pose_model.predict(pose_X)[0]\n",
    "        pose_prob = pose_model.predict_proba(pose_X)[0]\n",
    "#            print(pose_class, pose_prob)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return face_class, pose_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fa9f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features_from_video(video_file, face_model, pose_model):\n",
    "    \"\"\"\n",
    "    Extract features from a video using the body_detect function.\n",
    "    \n",
    "    Args:\n",
    "    - video_file (str): Path to the video file.\n",
    "    - body_detect (func): Function to detect features in an image frame.\n",
    "    \n",
    "    Returns:\n",
    "    - List[Tuple[float, Any]]: List of timestamps and their detected features.\n",
    "    \"\"\"\n",
    "    # List to store the results\n",
    "    results = []\n",
    "    curr_features = ('','')\n",
    "    # Open the video\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    \n",
    "    # Check if video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Couldn't open video.\")\n",
    "        return results\n",
    "    \n",
    "    # Get the frames per second of the video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    while True:\n",
    "        # Read the next frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Break the loop if video has ended\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Get the current position in the video in seconds\n",
    "        timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0\n",
    "        \n",
    "        # Detect features in the frame\n",
    "        features = feature_detect(frame, face_model, pose_model)\n",
    "        if curr_features != features:\n",
    "            curr_features = features\n",
    "            print(timestamp, features)\n",
    "            # Append the timestamp and features to the results list\n",
    "            results.append((timestamp, features))\n",
    "    \n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# def body_detect(frame):\n",
    "#     # Your detection code here\n",
    "#     return detected_features\n",
    "\n",
    "# video_file = 'path_to_video.mp4'\n",
    "# features_list = extract_features_from_video(video_file, body_detect)\n",
    "# print(features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad2d22b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 ('frown', 'hands pocket')\n",
      "1.8998100000000002 ('smile', 'hands pocket')\n",
      "4.09959 ('smile', 'arms crossed')\n",
      "5.26614 ('smile', 'hands pocket')\n",
      "5.332800000000001 ('smile', 'arms crossed')\n",
      "5.432790000000001 ('smile', 'hands pocket')\n",
      "5.46612 ('smile', 'arms crossed')\n",
      "5.6661 ('smile', 'hands pocket')\n",
      "5.732760000000001 ('smile', 'arms crossed')\n",
      "5.76609 ('smile', 'hands pocket')\n",
      "5.832750000000001 ('smile', 'arms crossed')\n",
      "6.332700000000001 ('smile', 'hands pocket')\n",
      "6.699330000000001 ('smile', 'arms crossed')\n",
      "6.999300000000001 ('smile', 'hands pocket')\n",
      "7.03263 ('smile', 'arms crossed')\n",
      "10.19898 ('smile', 'hands pocket')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 33\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# cap = cv2.VideoCapture(video_path)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# if not cap.isOpened():\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     cap.release()\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     cv2.destroyAllWindows()\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     face_model, pose_model \u001b[38;5;241m=\u001b[39m feature_detect_setup()\n\u001b[0;32m---> 33\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_from_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(results)\n",
      "Cell \u001b[0;32mIn[21], line 38\u001b[0m, in \u001b[0;36mextract_features_from_video\u001b[0;34m(video_file, face_model, pose_model)\u001b[0m\n\u001b[1;32m     35\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_POS_MSEC) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000.0\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Detect features in the frame\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_detect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_features \u001b[38;5;241m!=\u001b[39m features:\n\u001b[1;32m     40\u001b[0m     curr_features \u001b[38;5;241m=\u001b[39m features\n",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m, in \u001b[0;36mfeature_detect\u001b[0;34m(frame, face_model, pose_model)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Make Detections\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mp_holistic\u001b[38;5;241m.\u001b[39mHolistic(min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m holistic:\n\u001b[0;32m---> 21\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mholistic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Extract Pose landmarks\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     pose \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/mediapipe/python/solutions/holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/mediapipe/python/solution_base.py:372\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    366\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    368\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    369\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    370\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "# Open the video file\n",
    "    video_file = '/Users/kailashambwani/Dropbox/PitchPerfect/ray_video.mov'\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# if not cap.isOpened():\n",
    "#     print(\"Error: Couldn't open video.\")\n",
    "# else:\n",
    "#     while True:\n",
    "#         # Read the next frame from the video\n",
    "#         ret, frame = cap.read()\n",
    "        \n",
    "#         # If ret is False, then we've reached the end of the video\n",
    "#         if not ret:\n",
    "#             break\n",
    "        \n",
    "#         # Now, 'frame' contains the current frame and you can process it if needed\n",
    "#         # For demonstration, let's display the frame\n",
    "#         cv2.imshow(\"Frame\", frame)\n",
    "        \n",
    "#         # Wait for 25ms, and then display the next frame. \n",
    "#         # This will play the video at a normal speed.\n",
    "#         # If you press 'q', it will exit the loop and stop displaying the video.\n",
    "#         if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#     # Release the video capture object and close all OpenCV windows\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "    face_model, pose_model = feature_detect_setup()\n",
    "    results = extract_features_from_video(video_file, face_model, pose_model)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f0c16fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b915cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
